# Load dataset
fire_data <- read.csv("forestfires.csv")
# Explore the dataset
str(fire_data)  # Overview of the dataset
summary(fire_data)  # Summary of all variables
# Visualize the distribution of the target variable (Burned Area)
ggplot(fire_data, aes(x = area)) +
geom_histogram(bins = 30, fill = 'blue', color = 'black') +
labs(title = "Distribution of Burned Area", x = "Burned Area (ha)", y = "Frequency")
# Log transformation of the target variable (to deal with skewness)
fire_data$log_area <- log1p(fire_data$area)
# Visualizing the log-transformed target variable
ggplot(fire_data, aes(x = log_area)) +
geom_histogram(bins = 30, fill = 'green', color = 'black') +
labs(title = "Distribution of Log Transformed Burned Area", x = "Log(Burned Area)", y = "Frequency")
# Handling categorical variables: Converting 'month' and 'day' into factors
fire_data$month <- as.factor(fire_data$month)
fire_data$day <- as.factor(fire_data$day)
# Handling missing values (if any)
fire_data <- na.omit(fire_data)
# Correlation matrix (for numeric features)
numeric_vars <- fire_data %>%
select(FFMC, DMC, DC, ISI, temp, RH, wind, rain, area)
corr_matrix <- cor(numeric_vars)
corrplot(corr_matrix, method = "color", addCoef.col = "black", number.cex = 0.7)
# Feature Engineering: Converting categorical variables into dummy variables
fire_data <- model.matrix(~ . - 1, data = fire_data)
# Data Scaling
preProcValues <- preProcess(fire_data[, -ncol(fire_data)], method = c("center", "scale"))
fire_data_scaled <- predict(preProcValues, fire_data)
# Train-Test Split
set.seed(123)
split <- sample.split(fire_data_scaled$log_area, SplitRatio = 0.7)
train_data <- subset(fire_data_scaled, split == TRUE)
test_data <- subset(fire_data_scaled, split == FALSE)
# Model 1: Linear Regression
lm_model <- lm(log_area ~ ., data = train_data)
summary(lm_model)
# Predictions on test set
lm_predictions <- predict(lm_model, newdata = test_data)
# Model Evaluation for Linear Regression
mse_lm <- mean((lm_predictions - test_data$log_area)^2)
cat("MSE for Linear Regression:", mse_lm, "\n")
# Model 2: Random Forest
rf_model <- randomForest(log_area ~ ., data = train_data, ntree = 500, mtry = 3, importance = TRUE)
rf_predictions <- predict(rf_model, newdata = test_data)
# Model Evaluation for Random Forest
mse_rf <- mean((rf_predictions - test_data$log_area)^2)
cat("MSE for Random Forest:", mse_rf, "\n")
# Feature Importance for Random Forest
varImpPlot(rf_model)
# Cross-validation using Caret
train_control <- trainControl(method = "cv", number = 10)
lm_cv <- train(log_area ~ ., data = train_data, method = "lm", trControl = train_control)
rf_cv <- train(log_area ~ ., data = train_data, method = "rf", trControl = train_control)
# Cross-validated results
print(lm_cv)
print(rf_cv)
# Hyperparameter Tuning for Random Forest
tune_grid <- expand.grid(mtry = c(2, 3, 4, 5))
rf_tuned <- train(log_area ~ ., data = train_data, method = "rf", tuneGrid = tune_grid, trControl = train_control)
# Final Results
rf_tuned_predictions <- predict(rf_tuned, newdata = test_data)
mse_rf_tuned <- mean((rf_tuned_predictions - test_data$log_area)^2)
cat("MSE for Tuned Random Forest:", mse_rf_tuned, "\n")
# Visualize Predictions vs Actual
ggplot() +
geom_point(aes(x = test_data$log_area, y = lm_predictions), color = 'blue', alpha = 0.5) +
geom_point(aes(x = test_data$log_area, y = rf_predictions), color = 'red', alpha = 0.5) +
geom_point(aes(x = test_data$log_area, y = rf_tuned_predictions), color = 'green', alpha = 0.5) +
labs(title = "Predicted vs Actual (Log Transformed Burned Area)", x = "Actual", y = "Predicted") +
theme_minimal()
# Model Comparison Table
model_comparison <- data.frame(
Model = c("Linear Regression", "Random Forest", "Tuned Random Forest"),
MSE = c(mse_lm, mse_rf, mse_rf_tuned)
)
print(model_comparison)
# Optional: Lasso Regression for Feature Selection
x <- as.matrix(train_data[, -ncol(train_data)])
y <- train_data$log_area
lasso_model <- cv.glmnet(x, y, alpha = 1)
coef(lasso_model, s = "lambda.min")
# Lasso Predictions
lasso_predictions <- predict(lasso_model, newx = as.matrix(test_data[, -ncol(test_data)]), s = "lambda.min")
mse_lasso <- mean((lasso_predictions - test_data$log_area)^2)
cat("MSE for Lasso Regression:", mse_lasso, "\n")
# Install necessary packages
install.packages("ggplot2")
install.packages("dplyr")
install.packages("caret")
install.packages("randomForest")
install.packages("MASS")
install.packages("corrplot")
install.packages("caTools")
install.packages("glmnet")
# After installing, you can load the libraries and run your code.
clear
library(glmnet)
install.packages("glmnet")
library(glmnet)
fire_data <- read.csv("forestfires.csv")
# Load dataset
fire_data <- read.csv("D:\Deepak\forest+fires\forestfires.csv")
# Load dataset
fire_data <- read.csv(r"D:\Deepak\forest+fires\forestfires.csv")
# Load dataset
fire_data <- read.csv("forestfires.csv")
fire_data <- read.csv("forestfires.csv")
# Load dataset
fire_data <- read.csv("D:\\Deepak\\forestfires.csv")
View(fire_data)
View(fire_data)
head(fire_data, 5)
str(fire_data)  # Overview of the dataset
summary(fire_data)  # Summary of all variables
# Visualize the distribution of the target variable (Burned Area)
ggplot(fire_data, aes(x = area)) +
geom_histogram(bins = 30, fill = 'blue', color = 'black') +
labs(title = "Distribution of Burned Area", x = "Burned Area (ha)", y = "Frequency")
# Visualize the distribution of the target variable (Burned Area)
ggplot(fire_data, aes(x = area)) +
geom_histogram(bins = 30, fill = 'blue', color = 'black') +
labs(title = "Distribution of Burned Area", x = "Burned Area (ha)", y = "Frequency")
fires_by_month %>%
ggplot(aes(x = month, y = total_fires, fill = "Fire count")) +
geom_col() +
theme (
plot.title = element_text(size=15, face= "bold", colour= "black" ),
axis.title.x = element_text(size=14, face="bold", colour = "black"),
axis.title.y = element_text(size=14, face="bold", colour = "black"),
axis.text.x = element_text(size=12, face="bold", colour = "black"),
axis.text.y = element_text(size=12, face="bold", colour = "black")
) +
labs(
title = "Number of forest fires in data by month",
y = "Fire count",
x = "Month",
fill = "Legend"
)
library("datasets", lib.loc="C:/Users/HASNAIN/anaconda3/envs/rstudio/lib/R/library")
detach("package:datasets", unload=TRUE)
fire_data$log_area <- log1p(fire_data$area)
head(fire_data, 5)
fire_data$month <- as.factor(fire_data$month)
fire_data$day <- as.factor(fire_data$day)
head(fire_data, 5)
corrplot(corr_matrix, method = "color", addCoef.col = "black", number.cex = 0.7)
# Load necessary libraries
library(ggplot2)
library(dplyr)
library(caret)
library(randomForest)
library(MASS)
library(corrplot)
library(gridExtra)
library(caTools)
library(glmnet)
install.packages("MASS")
numeric_vars <- fire_data %>%
select(FFMC, DMC, DC, ISI, temp, RH, wind, rain, area)
corr_matrix <- cor(numeric_vars)
corrplot(corr_matrix, method = "color", addCoef.col = "black", number.cex = 0.7)
install.packages("dplyr")
library(dplyr)
install.packages("dplyr")
library(dplyr)
library("dplyr", lib.loc="C:/Users/HASNAIN/anaconda3/envs/rstudio/lib/R/library")
detach("package:dplyr", unload=TRUE)
library("dplyr", lib.loc="C:/Users/HASNAIN/anaconda3/envs/rstudio/lib/R/library")
install.packages("dplyr")
# Select the numeric variables from fire_data
numeric_vars <- fire_data %>%
select(FFMC, DMC, DC, ISI, temp, RH, wind, rain, area)
# Calculate the correlation matrix
corr_matrix <- cor(numeric_vars)
# Plot the correlation matrix using corrplot
corrplot(corr_matrix, method = "color", addCoef.col = "black", number.cex = 0.7)
library("corrplot", lib.loc="C:/Users/HASNAIN/anaconda3/envs/rstudio/lib/R/library")
# Select the numeric variables from fire_data
numeric_vars <- fire_data %>%
select(FFMC, DMC, DC, ISI, temp, RH, wind, rain, area)
# Calculate the correlation matrix
corr_matrix <- cor(numeric_vars)
# Plot the correlation matrix using corrplot
corrplot(corr_matrix, method = "color", addCoef.col = "black", number.cex = 0.7)
# Data Scaling
preProcValues <- preProcess(fire_data[, -ncol(fire_data)], method = c("center", "scale"))
fire_data_scaled <- predict(preProcValues, fire_data)
# Data Scaling
preProcValues <- preProcess(fire_data[, -ncol(fire_data)], method = c("center", "scale"))
library("dplyr", lib.loc="C:/Users/HASNAIN/anaconda3/envs/rstudio/lib/R/library")
install.packages("pillar")
library("corrplot", lib.loc="C:/Users/HASNAIN/anaconda3/envs/rstudio/lib/R/library")
library("dplyr", lib.loc="C:/Users/HASNAIN/anaconda3/envs/rstudio/lib/R/library")
install.packages("pillar")
install.packages("cli")
install.packages("lifecycle")
install.packages("rlang")
library(caret)
library(grDevices)
library(leaps)
library(relaimpo)
library(corrplot)
library(car)
library(DAAG)
# Alters the way numbers are outputted
options(scipen = 999,digits = 3)
install.packages("caret")
install.packages("leaps")
install.packages("relaimpo")
install.packages("car")
install.packages("DAAG")
library(caret)
library(grDevices)
library(leaps)
library(relaimpo)
library(corrplot)
library(car)
library(DAAG)
# Alters the way numbers are outputted
options(scipen = 999,digits = 3)
dim(fire_data)
library(ggplot2)
library(dplyr)
library(caret)
library(randomForest)
library(MASS)
library(corrplot)
library(gridExtra)
library(caTools)
library(glmnet)
library("corrplot", lib.loc="C:/Users/HASNAIN/anaconda3/envs/rstudio/lib/R/library")
colnames(forestdata)
colnames(fire_data)
library("dplyr", lib.loc="C:/Users/HASNAIN/anaconda3/envs/rstudio/lib/R/library")
View(fire_data)
# Explore the dataset
str(fire_data)  # Overview of the dataset
summary(fire_data)  # Summary of all variables
colnames(fire_data)
fire_data$log_area <- log1p(fire_data$area)
dim(fire_data)
fire_data <- read.csv("D:\\Deepak\\forestfires.csv")
dim(fire_data)
colnames(fire_data)
sum(is.na(fire_data))
# Select only the numeric columns
numeric_vars <- fire_data %>%
select(FFMC, DMC, DC, ISI, temp, RH, wind, rain, area)
# Create boxplots for all numeric variables
boxplot(numeric_vars, main = "Boxplot of Numeric Variables", las = 2)
numeric_vars <- fire_data %>% select(FFMC, DMC, DC, ISI, temp, RH, wind, rain, area)
# Function to detect outliers using IQR method
detect_outliers <- function(x) {
Q1 <- quantile(x, 0.25)
Q3 <- quantile(x, 0.75)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
return(x < lower_bound | x > upper_bound)
}
# Apply the function to each numeric column
outliers <- sapply(numeric_vars, detect_outliers)
# Check the number of outliers in each column
colSums(outliers)
outliers <- sapply(numeric_vars, detect_outliers)
# Select only the numeric columns from fire_data
numeric_vars <- fire_data %>%
select(FFMC, DMC, DC, ISI, temp, RH, wind, rain, area)
install.packages("dplyr")
install.packages("dplyr")
library("dbplyr", lib.loc="C:/Users/HASNAIN/anaconda3/envs/rstudio/lib/R/library")
install.packages("pillar")
install.packages("pillar")
# Select only the numeric columns from fire_data
numeric_vars <- fire_data %>%
select(FFMC, DMC, DC, ISI, temp, RH, wind, rain, area)
install.packages("pillar")
# Select the numeric columns using base R
numeric_vars <- fire_data[, c("FFMC", "DMC", "DC", "ISI", "temp", "RH", "wind", "rain", "area")]
# Function to detect outliers using IQR method
detect_outliers <- function(x) {
Q1 <- quantile(x, 0.25)
Q3 <- quantile(x, 0.75)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
return(x < lower_bound | x > upper_bound)
}
# Apply the function to each numeric column
outliers <- sapply(numeric_vars, detect_outliers)
# Check the number of outliers in each column
colSums(outliers)
show(outliers)
boxplot(numeric_vars, main = "Boxplot of Numeric Variables", las = 2)
corr_matrix <- cor(numeric_vars)
corrplot(corr_matrix, method = "color", addCoef.col = "black", number.cex = 0.7)
library("corrplot", lib.loc="C:/Users/HASNAIN/anaconda3/envs/rstudio/lib/R/library")
corr_matrix <- cor(numeric_vars)
corrplot(corr_matrix, method = "color", addCoef.col = "black", number.cex = 0.7)
install.packages("GGally")
# Install and load GGally for pair plots
if(!require(GGally)) install.packages("GGally")
library(GGally)
# Create a pair plot for the numeric variables
ggpairs(numeric_vars)
# Convert month and day string variables into numeric values
fire_data$month <- as.numeric(as.factor(fire_data$month))
fire_data$day <- as.numeric(as.factor(fire_data$day))
head(fire_data, 5)
fire_data$log_area <- log1p(fire_data$area)
head(fire_data, 5)
# Function to replace outliers with median
replace_outliers <- function(x) {
Q1 <- quantile(x, 0.25)
Q3 <- quantile(x, 0.75)
IQR <- Q3 - Q1
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR
x[x < lower_bound | x > upper_bound] <- median(x)
return(x)
}
# Apply the function to each numeric column
numeric_vars_imputed <- as.data.frame(lapply(numeric_vars, replace_outliers))
boxplot(numeric_vars_imputed, main = "Boxplot of Numeric Variables", las = 2)
boxplot(numeric_vars_imputed, main = "Boxplot of Numeric Variables After Imputation", las = 2)
outliers <- sapply(numeric_vars_imputed, detect_outliers)
colSums(outliers)
numeric_vars <- numeric_vars[, !colnames(numeric_vars) %in% "area"]
head(fire_data, 5)
fire_data <- fire_data[, !colnames(fire_data) %in% "area"]
head(fire_data, 5)
# Data Scaling
preProcValues <- preProcess(fire_data[, -ncol(fire_data)], method = c("center", "scale"))
fire_data_scaled <- predict(preProcValues, fire_data)
# Remove the 'log_area' column as it's the last column in the dataset.
fire_data_no_log_area <- fire_data[, -ncol(fire_data)]
# Calculate column-wise means and standard deviations
means <- apply(fire_data_no_log_area, 2, mean)
sds <- apply(fire_data_no_log_area, 2, sd)
# Scale the data: (value - mean) / sd
fire_data_scaled <- as.data.frame(scale(fire_data_no_log_area, center = means, scale = sds))
# Add the 'log_area' column back
fire_data_scaled$log_area <- fire_data$log_area
set.seed(123)
split <- sample.split(fire_data_scaled$log_area, SplitRatio = 0.7)
train_data <- subset(fire_data_scaled, split == TRUE)
test_data <- subset(fire_data_scaled, split == FALSE)
library("caTools", lib.loc="C:/Users/HASNAIN/anaconda3/envs/rstudio/lib/R/library")
set.seed(123)
split <- sample.split(fire_data_scaled$log_area, SplitRatio = 0.7)
train_data <- subset(fire_data_scaled, split == TRUE)
test_data <- subset(fire_data_scaled, split == FALSE)
library("caret", lib.loc="C:/Users/HASNAIN/anaconda3/envs/rstudio/lib/R/library")
detach("package:caret", unload=TRUE)
head(fire_data_scaled, 5)
library("caret", lib.loc="C:/Users/HASNAIN/anaconda3/envs/rstudio/lib/R/library")
install.packages("ggplot2")
library("caret", lib.loc="C:/Users/HASNAIN/anaconda3/envs/rstudio/lib/R/library")
head(fire_data_scaled, 5)
# Model 1: Linear Regression
lm_model <- lm(log_area ~ ., data = train_data)
summary(lm_model)
# Predictions on test set
lm_predictions <- predict(lm_model, newdata = test_data)
# Model Evaluation for Linear Regression
mse_lm <- mean((lm_predictions - test_data$log_area)^2)
cat("MSE for Linear Regression:", mse_lm, "\n")
# Model 2: Random Forest
rf_model <- randomForest(log_area ~ ., data = train_data, ntree = 500, mtry = 3, importance = TRUE)
rf_predictions <- predict(rf_model, newdata = test_data)
# Model Evaluation for Random Forest
mse_rf <- mean((rf_predictions - test_data$log_area)^2)
cat("MSE for Random Forest:", mse_rf, "\n")
# Model 2: Random Forest
rf_model <- randomForest(log_area ~ ., data = train_data, ntree = 500, mtry = 3, importance = TRUE)
rf_predictions <- predict(rf_model, newdata = test_data)
library(randomForest)
varImpPlot(lm_model)
install.packages("randomForest")
install.packages("ranger")
install.packages("ranger")
install.packages("RcppEigen")
install.packages("https://cran.r-project.org/src/contrib/Archive/randomForest/randomForest_4.6-14.tar.gz", repos = NULL, type = "source")
install.packages("randomForest")
install.packages("randomForestExplainer")
install.packages("ranger")
library("randomForest", lib.loc="C:/Users/HASNAIN/anaconda3/envs/rstudio/lib/R/library")
# Model 2: Random Forest
rf_model <- randomForest(log_area ~ ., data = train_data, ntree = 500, mtry = 3, importance = TRUE)
rf_predictions <- predict(rf_model, newdata = test_data)
# Model Evaluation for Random Forest
mse_rf <- mean((rf_predictions - test_data$log_area)^2)
cat("MSE for Random Forest:", mse_rf, "\n")
# Model 2: Random Forest
rf_model <- randomForest(log_area ~ ., data = train_data, ntree = 1000, mtry = 3, importance = TRUE)
rf_predictions <- predict(rf_model, newdata = test_data)
# Model Evaluation for Random Forest
mse_rf <- mean((rf_predictions - test_data$log_area)^2)
cat("MSE for Random Forest:", mse_rf, "\n")
# Model 2: Random Forest
rf_model <- randomForest(log_area ~ ., data = train_data, ntree = 1500, mtry = 3, importance = TRUE)
rf_predictions <- predict(rf_model, newdata = test_data)
# Model Evaluation for Random Forest
mse_rf <- mean((rf_predictions - test_data$log_area)^2)
cat("MSE for Random Forest:", mse_rf, "\n")
# Model 2: Random Forest
rf_model <- randomForest(log_area ~ ., data = train_data, ntree = 500, mtry = 3, importance = TRUE)
rf_predictions <- predict(rf_model, newdata = test_data)
# Model Evaluation for Random Forest
mse_rf <- mean((rf_predictions - test_data$log_area)^2)
cat("MSE for Random Forest:", mse_rf, "\n")
varImpPlot(rf_model)
library("caret", lib.loc="C:/Users/HASNAIN/anaconda3/envs/rstudio/lib/R/library")
library("caret", lib.loc="C:/Users/HASNAIN/anaconda3/envs/rstudio/lib/R/library")
library("ranger", lib.loc="C:/Users/HASNAIN/anaconda3/envs/rstudio/lib/R/library")
library("randomForest", lib.loc="C:/Users/HASNAIN/anaconda3/envs/rstudio/lib/R/library")
# Load ranger for Random Forest
library(ranger)
# Initialize a vector to store errors for each fold
cv_errors_rf <- numeric(k)
# Perform k-fold cross-validation for random forest
for (i in 1:k) {
# Split data into training and validation sets
train_fold <- train_data[folds != i, ]
test_fold <- train_data[folds == i, ]
# Fit the random forest model
rf_model <- ranger(log_area ~ ., data = train_fold, num.trees = 500, mtry = 3)
# Predict on the validation set
predictions <- predict(rf_model, data = test_fold)$predictions
# Compute the mean squared error (MSE) for the fold
mse <- mean((test_fold$log_area - predictions)^2)
# Store the error for this fold
cv_errors_rf[i] <- mse
}
# Average MSE across all folds
mean_cv_error_rf <- mean(cv_errors_rf)
print(mean_cv_error_rf)
# Cross-validation using Caret
train_control <- trainControl(method = "cv", number = 10)
lm_cv <- train(log_area ~ ., data = train_data, method = "lm", trControl = train_control)
rf_cv <- train(log_area ~ ., data = train_data, method = "rf", trControl = train_control)
# Cross-validated results
print(lm_cv)
print(rf_cv)
model_comparison <- data.frame(
Model = c("Linear Regression", "Random Forest"),
MSE = c(mse_lm, mse_rf)
)
print(model_comparison)
varImpPlot(lm_model)
# Optional: Lasso Regression for Feature Selection
x <- as.matrix(train_data[, -ncol(train_data)])
y <- train_data$log_area
lasso_model <- cv.glmnet(x, y, alpha = 1)
coef(lasso_model, s = "lambda.min")
# Lasso Predictions
lasso_predictions <- predict(lasso_model, newx = as.matrix(test_data[, -ncol(test_data)]), s = "lambda.min")
mse_lasso <- mean((lasso_predictions - test_data$log_area)^2)
cat("MSE for Lasso Regression:", mse_lasso, "\n")
library("glmnet", lib.loc="C:/Users/HASNAIN/anaconda3/envs/rstudio/lib/R/library")
# Optional: Lasso Regression for Feature Selection
x <- as.matrix(train_data[, -ncol(train_data)])
y <- train_data$log_area
lasso_model <- cv.glmnet(x, y, alpha = 1)
coef(lasso_model, s = "lambda.min")
# Lasso Predictions
lasso_predictions <- predict(lasso_model, newx = as.matrix(test_data[, -ncol(test_data)]), s = "lambda.min")
mse_lasso <- mean((lasso_predictions - test_data$log_area)^2)
cat("MSE for Lasso Regression:", mse_lasso, "\n")
model_comparison <- data.frame(
Model = c("Linear Regression", "Random Forest", "Lasso"),
MSE = c(mse_lm, mse_rf, mse_lasso)
)
print(model_comparison)
# Optional: Lasso Regression for Feature Selection
x <- as.matrix(train_data[, -ncol(train_data)])
y <- train_data$log_area
lasso_model <- cv.glmnet(x, y, alpha = 0.1)
coef(lasso_model, s = "lambda.min")
# Lasso Predictions
lasso_predictions <- predict(lasso_model, newx = as.matrix(test_data[, -ncol(test_data)]), s = "lambda.min")
mse_lasso <- mean((lasso_predictions - test_data$log_area)^2)
cat("MSE for Lasso Regression:", mse_lasso, "\n")
# Optional: Lasso Regression for Feature Selection
x <- as.matrix(train_data[, -ncol(train_data)])
y <- train_data$log_area
lasso_model <- cv.glmnet(x, y, alpha = 0.01)
coef(lasso_model, s = "lambda.min")
# Lasso Predictions
lasso_predictions <- predict(lasso_model, newx = as.matrix(test_data[, -ncol(test_data)]), s = "lambda.min")
mse_lasso <- mean((lasso_predictions - test_data$log_area)^2)
cat("MSE for Lasso Regression:", mse_lasso, "\n")
# Optional: Lasso Regression for Feature Selection
x <- as.matrix(train_data[, -ncol(train_data)])
y <- train_data$log_area
lasso_model <- cv.glmnet(x, y, alpha = 2)
coef(lasso_model, s = "lambda.min")
# Lasso Predictions
lasso_predictions <- predict(lasso_model, newx = as.matrix(test_data[, -ncol(test_data)]), s = "lambda.min")
mse_lasso <- mean((lasso_predictions - test_data$log_area)^2)
cat("MSE for Lasso Regression:", mse_lasso, "\n")
# Optional: Lasso Regression for Feature Selection
x <- as.matrix(train_data[, -ncol(train_data)])
y <- train_data$log_area
lasso_model <- cv.glmnet(x, y, alpha = 5)
coef(lasso_model, s = "lambda.min")
# Lasso Predictions
lasso_predictions <- predict(lasso_model, newx = as.matrix(test_data[, -ncol(test_data)]), s = "lambda.min")
mse_lasso <- mean((lasso_predictions - test_data$log_area)^2)
cat("MSE for Lasso Regression:", mse_lasso, "\n")
